
---
title: "The Effect of Total Variation on R-squared"
output: html_notebook
---

```{r setup, include=FALSE, message=FALSE}
rm(list = ls())
library(tidyverse)
library(gridExtra)
library(kableExtra)
library(latex2exp)
```

### Introduction

Regression is a statistical method used to model facts we observe in the world.  The purpose of producing such models is to help us explain those facts.  For example, how can we explain, or model, the fact that arctic ice is melting or that temperatures are rising? Regression uses the data that is measured alongside a fact of interest to model the latter by looking at the relationship that may exist between. In the case of a linear regression model, as its name suggests, the model seeks to exploit the fact that the independent explanatory variables and the dependent variable of interest are linearly correlated. As one variable increases or decreases so does the other. Both variables are then said to be positively, or negatively, linearly correlated. 

The linear relationship between a dependent and an independent variable is represented in the form of the line $y = \beta_0 + \beta_1x + \epsilon$, where the regression coefficients $\beta_0$ (called *intercept*) and $\beta_1$ (called *slope*) represent parameter estimates to be computed. The error term $\epsilon$ measures the differences between observed and predicted data points of $y$. Such linear regression models are evaluated using metrics such as the standard error of the regression se, the correlation coefficient r, the coefficient of determination R2, and the adjusted coefficient of determination adjusted R2. The discussion proposed in this artile is limited to R2. 

The goal of the article is to explore the evaluation metric R2 in light of the variance that is inherent to a dependent variable of interest. The question posed is whether the variance of a dependent variable y affect the ability of an independant variable to explain their linear relationship? Or, in other words, is a dependent variable better suited to a linear regression model if its variance is small rather than large? 

Intuitively the answer is yes of course as low variance essentially means that the observed values of y will be scattered more closely around the regression line. Consequently, assuming that some linear relationship exists, the overall error will be smaller. But that is just inutuition and in this article I propose to demonstrate the same using random data and R (the programming language). 


### The deterministic and stochastic components of regression

A good way to think about a regression model is in terms of their deterministic and and stochastic components ^[https://statisticsbyjim.com/regression/check-residual-plots-regression-analysis/]. The term deterministic relates to the philosophical doctrine that all events, including human action, are ultimately determined by causes regarded as external to the will ^[https://www.lexico.com/en/definition/deterministic]. The term stochastic relates to having a random probability distribution or pattern that may be analysed statistically but may not be predicted precisely ^[https://www.lexico.com/en/definition/stochastic]. In the context of linear regression model, the regression coefficients represent the deterministic, explainable, part, the error represents the stochastic, random, part and and the model can be described as follows:

$$Dependent \ Variable = Deterministic + Stochastic$$

The cornerstone of the R2 metric is that variation in $y$, when seen through the lens of its correlation with $x$, is composed of both deterministic and stochastic variation. The deterministic variation represents that which is explained by $x$, the stochastic variation represents that which $x$ cannot explain. If there were no stochastic variation, the error term of the model (see algebraic formulation above) would be zero, meaning that $y$ and $x$ are perfectly correlated and one could know or predict one by knowing the other. An example would be the length of a side of a square to the perimeter of the square ^[https://www.quora.com/What-is-a-perfect-positive-correlation]. The purpose of R2 is to determine the proportion of the deterministic variation in the total variation of $y$. 

In order to determine the proportions of explainable and unexplainable variation three different distances are measured:

* the distances between observed values and the mean of observed values: $\sum{(y-\bar{y})^2}$.  
* the distances between predicted values and the mean of observed values: $\sum{(\hat{y}-\bar{y})^2}$.
* the distances between predicted values and corresponding observed values: $\sum{(\hat{y} - y)^2}$.

Distances are squared and summed to produce The total variation *SSTot*, variation due to regression *SSReg* and the variation due to residuals *SSResid*. Values are squared simply to eliminate the fact that distances may be positive or negative depending on whether observed values lie below or above the regression line. As seen above, the variation due to regression (deterministic) and the variation due to residuals (stochastic) are both based on the predicted values produced by the model. Total variation is the sum of both:

$$SSTot = SSReg + SSResid$$

Algebraically this is expressed as follows:

$$\sum{(y-\bar{y})^2} = \sum{(\hat{y}-\bar{y})^2}  + \sum{(\hat{y} - y)^2}$$

$R^2$ is the ratio of SSReg to SSTot:

$$R^2 = \frac{SSReg}{SSTot} = 1 - \frac{SSResid}{SSTot}$$

R2 tells us how much of the model, in percent, can be attributed to the deterministic part of the linear relationship between $y$ and $x$ and, by extension, how much can be attributed to the stochastic part i.e. that which cannot be described by the linear model. 


<br />

### Demonstration

As mentioned above, let us now attempt to answer the question of interest by performing a small test using random data and R. The test consists of two linear regression models. Each model is based on a positive linear relationship between a dependent variable y and an independent variable x. The models differ only in the variation of y. All other factors are controlled for i.e. are identical.

#### Setting up the Data  

To observe whether the variance of the dependent variable y affect the ability of an independant variable to explain their linear relationship, two datasets with the following properties are compared:

* the datsets are composed of 100 random numbers drawn from a normal distribution. 
* both datasets have a mean of zero and a standard deviation of 1

The visualisations below include the density plots for each variable and each dataset are shown below and the scatterplots for each pair of y~x relationship. The scatterplots also show the regression lines (in blue) as well as the mean $\bar{y}$.

```{r, fig.width=10,fig.height=4}

# create the same random numbers
set.seed(123)

# helper data structures
plist = list()
plist_density = list()
data <- list()
rsquared <- vector()

# set up (shared) independent var
# x <- rnorm(100, mean=0, sd=1)

# multiplier for generating high/low variance in y
m = c(0.3, 1)

for (i in seq_along(m)){
  
  # set up dependent var
  x <- rnorm(100, mean=0, sd=1)
  y <- x + rnorm(100, mean=0, sd=i)# * m[[i]]
  ybar <- mean(y)
  data[[i]] <- data.frame(x,y)
  
  # produce linear model
  lm <- lm(data[[i]]$y ~ data[[i]]$x) # tilde is read as "is modeled as a function of".
  
  # get coefficients
  coeffs = round(coefficients(lm), 2)
  
  # get R-squared
  rsq = summary(lm)$r.squared
  rsquared <- c(rsquared, rsq)

  # setup annotations for displaying R-squared on plot
  annotations <- data.frame(
    xpos = -Inf, ypos = -Inf,
    annotateText = sprintf(TeX("$R^2$=%s", output = "character"), 
                           format(round(rsq, 2), nsmall = 2)),
    hjustvar = -0.2, vjustvar = -15)

  # set plot subtitle
  if (i==1) {subt="Low"} else {subt="High"}
  
  # setup scatterplot and add plot list
  plist[[i]] <- ggplot(data[[i]], aes(x=x, y=y)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, size=0.5) +
    geom_hline(yintercept=ybar, size=0.3) +
    geom_text(label=TeX("$\\bar{y}$"), x=2, y=ybar, vjust=0) +
    geom_text(data = annotations, 
              aes(x=xpos, y=ypos, hjust=hjustvar,
                  vjust=vjustvar,
                  label=annotateText),
              parse = TRUE) +
    ylim(-5,5) + 
    labs(title = paste(subt,"Variance"),
           subtitle = paste("Linear Model: y =", coeffs[1], "+", coeffs[2], "x"),
           caption = "", x = "x", y = "y",tag = "") +
    theme_minimal()
}

# produce linear model
low <- data[[1]]
high <- data[[2]]

for (i in 1:2){
  for(j in 1:2){
  plist_density[[i]] <- ggplot(data[[i]], aes(x=x, y=y)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, size=0.5) +
    geom_hline(yintercept=ybar, size=0.3) +
    geom_text(label=TeX("$\\bar{y}$"), x=2, y=ybar, vjust=0) +
    geom_text(data = annotations, 
              aes(x=xpos, y=ypos, hjust=hjustvar,
                  vjust=vjustvar,
                  label=annotateText),
              parse = TRUE) +
    ylim(-5,5) + 
    labs(title = paste(subt,"Variance"),
           subtitle = paste("Linear Model: y =", coeffs[1], "+", coeffs[2], "x"),
           caption = "", x = "x", y = "y",tag = "") +
    theme_minimal()  
  }
  }
 
# density plots 
d1 <- ggplot(data[[1]], aes(y)) + geom_density(color="darkblue", fill="lightblue") + xlim(-8,8) +  theme_minimal()
d2 <- ggplot(data[[2]], aes(y)) + geom_density(color="darkblue", fill="lightblue") + xlim(-8,8) + theme_minimal()
grid.arrange(d1,d2, ncol=3)

# display scatter plots 
margin = theme(plot.margin = unit(c(0,1,0,1), "cm"))
grid.arrange(grobs = lapply(plist, "+", margin), ncol=2)
```
<br />

#### Conmputing R2

In a first step, the values for SSReg, SSResid and SSTot are computed. These then allow us to compute $R^2$ and we see that the proportion of the total variation in the dependent variable $y$ that can be attributed to the linear relationship between $y$ and $x$ is `r round(rsquared[1]*100, 0)`% for the low variation data, and `r round(rsquared[2]*100, 0)`% for the high variation data respectively. These proportions are visualised in the stacked barcharts below. 

<br />

```{r}
# Compute Variations and R-squared

low <- data[[1]]
high <- data[[2]]

df <- data.frame("Dataset" = c("low var", "high var"),
                 "mean" = c(mean(low$y), mean(high$y)),
                 "variance" = c(var(low$y), var(high$y)))

# compute SSTot
df <- df %>% mutate(SSTot = c(sum((low$y - mean(low$y))**2),
                              sum((high$y - mean(high$y))**2)))

# compute SSReg
df <- df %>%
  mutate(SSReg = c(sum((predict(lm(low$y ~ low$x)) - mean(low$y))**2),
                   sum((predict(lm(high$y ~ high$x)) - mean(high$y))**2)))

# compute SSResid
df <- df %>%
  mutate(SSResid = c(sum((low$y - predict(lm(low$y ~ low$x)))**2),
                     sum((high$y - predict(lm(high$y ~ high$x)))**2)))

# compute R-squared
df <- df %>% mutate(RSquared = SSReg/SSTot*100)


# display dataframe
df %>%
  kable(digits = 1) %>%
  kable_styling(full_width = FALSE, position = 'left')
```

```{r, fig.width=5,fig.height=3}
# Visualise breakdown of SSTot

# pivot longer: prepare for stacked bar chart
tmp <- df %>%
  pivot_longer(c(SSReg, SSResid), names_to = "SS", values_to = "value") 

# setup & display plot
ggplot(tmp, aes(fill=SS, y=value, x=factor(Dataset))) +
geom_bar(position="stack", stat="identity") +
theme_minimal() +
theme(aspect.ratio = 2/(1+sqrt(5))) + 
theme(plot.title=element_text(size=12),
      axis.title=element_text(size=8)) +
labs(title = "SSReg vs. SSResid",
     fill = "", # hide legend title
     x = "", y = "Total Variation") + 
theme_minimal()
```

### Conclusion

In this article I have, hopefully, illustrated how a visual inspection of a scatterplot of a dependent variable $y$ and an independent variable $x$ can give a quick insight into how likely it is that $x$ is well suited to explaining $y$.

The comparison of the composition of SSTot for a low vs. a high total variation in $y$ about its mean, shows that in the former case the proportion of deterministic, explainable, variation is much larger than if the total variation in $y$. Indeed where $y$ exhibits a greater variation about its mean, we observe that less than half of the total variation can be attributed to the linear relationship. This suggests that the regressorvariable might not be well suited, at least not on its own, to explain changes in the regressand. 

Lastly, it is critical to understand that the stochastic part of the total variation in a dependant variable (i.e. the residuals) must be checked for true randomness. Random chance alone should determine the values of the error term, regardless of its magnitude. Residuals should be plotted and checked for patterns. If a pattern exists it means that there is explanatory power in the error^[https://statisticsbyjim.com/regression/check-residual-plots-regression-analysis/]. A detailed discussion of this aspect of regression analysis may be covered in a future article.

