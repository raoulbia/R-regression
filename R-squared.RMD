
---
title: "The Effect of Total Variation on R-squared"
output: html_notebook
---

```{r setup, include=FALSE, message=FALSE}
rm(list = ls())
library(tidyverse)
library(gridExtra)
library(kableExtra)
library(latex2exp)
```

### Introduction

This article deals with the evaluation of linear regression models. Evaluation metrics for linear regression models include the standard error of the regression se, the correlation coefficient r and the coefficient of determination R2. The focus of this article is R2, and the goal is to review how to "read" an R2 value when evaluating a linear regression model. Specifically, the article looks at how the total variation of a dependent variable, as a function of another independent variable, affects the value or R2.

The article is based on material learned as part of an introductory course in data analytics and various online resources, references to which are provided throughout the article. 

### A brief review of terminology and definitions

Regression is a statistical method used to model facts we observe in the world.  The purpose of producing such models is to help us explain those facts.  For example, how can we explain, or model, the fact that arctic ice is melting or temperatures are rising? 

In statistical parlance, the fact of interest that we wish to explain through a regression model is referred to as the regressand, or the dependant variable. The data that is used to help explain that fact consists of one or more variables, referred to as regressors, or independent variables.

Linear regression consists of finding a line (or a more complex linear function) that most closely fits the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes the line that minimizes the sum of squared distances between the true data and that line. This allows to estimate the conditional expectation (or population average value) of the dependent variable when the independent variables take on a given set of values ^[https://en.wikipedia.org/wiki/Regression_analysis].

In a linear relationship, as the regressor and regressand increase or decrease in tandem, and both variables are said to be positively or negatively correlated. The relationship between a dependent and one or more independent variables is represented in the form $y = \beta_0 + \beta_1x + \epsilon$, where the regression coefficients $\beta_0$ (called *intercept*) and $\beta_1$ (called *slope*) represent parameter estimates to be computed. The error term $\epsilon$ measures the differences between observed and predicted data points of $y$.

R2 is based on the concept of variation. In general, variation refers to how far a set of data points are from their mean. The avergae of this distance is referred to as variance. If data points are close to their mean value, the varaince is small, otherwise it is large. In the present context it is useful to remind ourselves that variation can take several forms. For example, y has its own variation when looked at in isolation i.e. its probability distribution. However, when looked at in the regressor-regressand context, the variation we are measuring is different. In that case it is the variation that arises from the correlation between y and x. If x is well suited to explaining y, then the variation in y will be low, high otherwise. This variation is independent of, and different from, the variation we observe when looking at y in isolation, without reference to another variable. 


### The deterministic and stochastic components of regression

A good way to think about a regression model is in terms of their deterministic and and stochastic components ^[https://statisticsbyjim.com/regression/check-residual-plots-regression-analysis/]. The term deterministic relates to the philosophical doctrine that all events, including human action, are ultimately determined by causes regarded as external to the will ^[https://www.lexico.com/en/definition/deterministic]. The term stochastic relates to having a random probability distribution or pattern that may be analysed statistically but may not be predicted precisely ^[https://www.lexico.com/en/definition/stochastic].

In the context of linear regression, the regression coefficients represent the deterministic, explainable, part of the relationship. The error represents the stochastic, random, part. We could therefore express a regression model as

$$Dependent \ Variable = Deterministic + Stochastic$$

### Decomposing Total Variation 

The cornerstone of the R2 metric is that variation in $y$, when seen through the lens of its correlation with $x$, is composed of both deterministic and stochastic variation. The deterministic variation represents that which is explained by $x$, the stochastic variation represents that which $x$ cannot explain. If there were no stochastic variation, the error term of the model (see algebraic formulation above) would be zero, meaning that $y$ and $x$ are perfectly correlated and one could know or predict one by knowing the other. An example would be the length of a side of a square to the perimeter of the square ^[https://www.quora.com/What-is-a-perfect-positive-correlation]. The purpose of R2 is to determine the proportion of the deterministic variation in the total variation of $y$. 

$R^2$ is based on three different measures of distance, involving observed values of $y$, their mean $\bar{y}$ and the values $\hat{y}$ predicted by the model:

* The total variation (SSTot), measured as the sum of squared (vertical) distances between observed values and the mean of observed values: $\sum{(y-\bar{y})^2}$.  
* The variation due to regression (SSReg), measured as the sum of squared (vertical) distances between predicted values and the mean of observed values: $\sum{(\hat{y}-\bar{y})^2}$.
* The variation due to residuals (SSResid), measured as the sum of squared (vertical) distances between predicted values and corresponding observed values: $\sum{(\hat{y} - y)^2}$.

Distances are measured as sum of squared distance. Values are squared simply to eliminate the fact that distances may be positive or negative depending on whether observed values lie below or above the regression line. The variation due to regression (deterministic) and the variation due to residuals (stochastic) are both based on the predicted values produced by the model. Total variation is the sum of variation due to regression and variation due to residuals:

$$SSTot = SSReg + SSResid$$

Algebraically this is expressed as follows:

$$\sum{(y-\bar{y})^2} = \sum{(\hat{y}-\bar{y})^2}  + \sum{(\hat{y} - y)^2}$$

$R^2$ is the ratio of SSReg to SSTot:

$$R^2 = \frac{SSReg}{SSTot} = 1 - \frac{SSResid}{SSTot}$$

R2 tells us how much of the model, in percent, can be attributed to the deterministic part of the linear relationship between $y$ and $x$ and, by extension, how much can be attributed to the stochastic part i.e. that which cannot be described by the linear model. 


<br />

### Demonstration

In this section I show some of the concepts discussed above using R. For the purposes of this article we will analyse a positive linear relationship between one dependent and one independent variable using randomly generated data.


#### Setting up the Data

To study the effect of total variation on the $R^2$ measure, two datasets composed of random numbers drawn from a normal distribution are compared. The primary difference in the datasets is their variance, low and high respectively, in the dependent variable $y$. Both datasets are shown in the scatterplots below. The plots also show the regression lines (in blue) as well as the mean $\bar{y}$ of the observed values $y$.

```{r, fig.width=10,fig.height=4}

# create the same random numbers
set.seed(123)

# helper data structures
plist = list()
data <- list()
rsquared <- vector()

# set up (shared) independent var
# x <- rnorm(100, mean=0, sd=1)

# multiplier for generating high/low variance in y
m = c(0.3, 1)

for (i in seq_along(m)){
  
  # set up dependent var
  x <- rnorm(100, mean=0, sd=1)
  y <- x + rnorm(100, mean=0, sd=i)# * m[[i]]
  ybar <- mean(y)
  data[[i]] <- data.frame(x,y)
  
  # produce linear model
  lm <- lm(data[[i]]$y ~ data[[i]]$x) # tilde is read as "is modeled as a function of".
  
  # get coefficients
  coeffs = round(coefficients(lm), 2)
  
  # get R-squared
  rsq = summary(lm)$r.squared
  rsquared <- c(rsquared, rsq)

  # setup annotations for displaying R-squared on plot
  annotations <- data.frame(
    xpos = -Inf, ypos = -Inf,
    annotateText = sprintf(TeX("$R^2$=%s", output = "character"), 
                           format(round(rsq, 2), nsmall = 2)),
    hjustvar = -0.2, vjustvar = -15)

  # set plot subtitle
  if (i==1) {subt="Low"} else {subt="High"}
  
  # setup scatterplot and add plot list
  plist[[i]] <- ggplot(data[[i]], aes(x=x, y=y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, size=0.5) +
  geom_hline(yintercept=ybar, size=0.3) +
  geom_text(label=TeX("$\\bar{y}$"), x=2, y=ybar, vjust=0) +
  geom_text(data = annotations, 
            aes(x=xpos, y=ypos, hjust=hjustvar,
                vjust=vjustvar,
                label=annotateText),
            parse = TRUE) +
  ylim(-5,5) + 
  labs(title = paste(subt,"Variance"),
         subtitle = paste("Linear Model: y =", coeffs[1], "+", coeffs[2], "x"),
         caption = "", x = "x", y = "y",tag = "") +
  theme_minimal()
}

# produce linear model
low <- data[[1]]
high <- data[[2]]

d1 <- ggplot(low, aes(y)) + geom_density(color="darkblue", fill="lightblue") + theme_minimal()
d2 <- ggplot(low, aes(x)) + geom_density(color="darkblue", fill="lightblue") + theme_minimal()
d3 <- ggplot(high, aes(y)) + geom_density(color="darkblue", fill="lightblue") + theme_minimal()
d4 <- ggplot(high, aes(x)) + geom_density(color="darkblue", fill="lightblue") + theme_minimal()
grid.arrange(d1,d2,d3,d4)
# plot(density(low$x))
# plot(density(high$y))
# plot(density(high$x))

# display plots 
margin = theme(plot.margin = unit(c(0,1,0,1), "cm"))
grid.arrange(grobs = lapply(plist, "+", margin), ncol=2)
```
<br />

#### Conmputing R2

In a first step, the values for SSReg, SSResid and SSTot are computed. These then allow us to compute $R^2$ and we see that the proportion of the total variation in the dependent variable $y$ that can be attributed to the linear relationship between $y$ and $x$ is `r round(rsquared[1]*100, 0)`% for the low variation data, and `r round(rsquared[2]*100, 0)`% for the high variation data respectively. These proportions are visualised in the stacked barcharts below. 

<br />

```{r}
# Compute Variations and R-squared

df <- data.frame("Dataset" = c("low var", "high var"),
                 "mean" = c(mean(low$y), mean(high$y)),
                 "variance" = c(var(low$y), var(high$y)))

# compute SSTot
df <- df %>% mutate(SSTot = c(sum((low$y - mean(low$y))**2),
                              sum((high$y - mean(high$y))**2)))

# compute SSReg
df <- df %>%
  mutate(SSReg = c(sum((predict(lm(low$y ~ low$x)) - mean(low$y))**2),
                   sum((predict(lm(high$y ~ high$x)) - mean(high$y))**2)))

# compute SSResid
df <- df %>%
  mutate(SSResid = c(sum((low$y - predict(lm(low$y ~ low$x)))**2),
                     sum((high$y - predict(lm(high$y ~ high$x)))**2)))

# compute R-squared
df <- df %>% mutate(RSquared = SSReg/SSTot*100)


# display dataframe
df %>%
  kable(digits = 1) %>%
  kable_styling(full_width = FALSE, position = 'left')
```

```{r, fig.width=5,fig.height=3}
# Visualise breakdown of SSTot

# pivot longer: prepare for stacked bar chart
tmp <- df %>%
  pivot_longer(c(SSReg, SSResid), names_to = "SS", values_to = "value") 

# setup & display plot
ggplot(tmp, aes(fill=SS, y=value, x=factor(Dataset))) +
geom_bar(position="stack", stat="identity") +
theme_minimal() +
theme(aspect.ratio = 2/(1+sqrt(5))) + 
theme(plot.title=element_text(size=12),
      axis.title=element_text(size=8)) +
labs(title = "SSReg vs. SSResid",
     fill = "", # hide legend title
     x = "", y = "Total Variation") + 
theme_minimal()
```

### Conclusion

In this article I have, hopefully, illustrated how a visual inspection of a scatterplot of a dependent variable $y$ and an independent variable $x$ can give a quick insight into how likely it is that $x$ is well suited to explaining $y$.

The comparison of the composition of SSTot for a low vs. a high total variation in $y$ about its mean, shows that in the former case the proportion of deterministic, explainable, variation is much larger than if the total variation in $y$. Indeed where $y$ exhibits a greater variation about its mean, we observe that less than half of the total variation can be attributed to the linear relationship. This suggests that the regressorvariable might not be well suited, at least not on its own, to explain changes in the regressand. 

Lastly, it is critical to understand that the stochastic part of the total variation in a dependant variable (i.e. the residuals) must be checked for true randomness. Random chance alone should determine the values of the error term, regardless of its magnitude. Residuals should be plotted and checked for patterns. If a pattern exists it means that there is explanatory power in the error^[https://statisticsbyjim.com/regression/check-residual-plots-regression-analysis/]. A detailed discussion of this aspect of regression analysis may be covered in a future article.

